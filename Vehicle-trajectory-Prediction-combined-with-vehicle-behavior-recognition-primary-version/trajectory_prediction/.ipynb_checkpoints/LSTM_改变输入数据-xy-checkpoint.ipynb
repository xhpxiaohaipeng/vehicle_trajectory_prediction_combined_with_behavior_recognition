{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "torch.Size([128, 150, 8]) torch.Size([128, 150, 4])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.distributions as dist\n",
    "from trajectory_dataPrepare1 import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('ok')\n",
    "Training_generator, Test, Valid, WholeSet= get_dataloader(128)\n",
    "\n",
    "class NNPred(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size,batch_size, dropout=0.5):\n",
    "        super(NNPred, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.in2lstm = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size,num_layers=self.num_layers,bidirectional=False,batch_first=True,dropout =0.1)\n",
    "        self.in2bilstm = nn.Linear(input_size, hidden_size)\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size//2,num_layers=self.num_layers,bidirectional=True,batch_first=True,dropout =0.1)\n",
    "    \n",
    "        self.fc0 = nn.Linear(hidden_size,hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2,int(hidden_size/2))\n",
    "        self.in2out = nn.Linear(input_size, int(hidden_size/2))\n",
    "        self.fc2 = nn.Linear(int(hidden_size/2) ,output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        #input = tensor shape[batchsize, len, num_features]\n",
    "        bilstm_out,_= self.bilstm(self.in2bilstm(input))\n",
    "        lstm_out,_= self.lstm(self.in2lstm(input))\n",
    "        out = self.tanh(self.fc0(lstm_out+bilstm_out))\n",
    "        out = self.tanh(self.fc1(out))\n",
    "        out =  out + self.in2out(input)\n",
    "        output = self.fc2(out)# range [0 -> 1 ]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 8, 5, 7]])\n"
     ]
    }
   ],
   "source": [
    "a = [[1,3,4,5]]\n",
    "b = [[4,5,1,2]]\n",
    "c = torch.tensor(a)+torch.tensor(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 2) (1, 1, 3)\n",
      "tensor([[[10, 14]]])\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "import numpy as np\n",
    "f = [[[2,3],[4,5],[4,6]]]\n",
    "d = [[[1,1,1]]]\n",
    "print(np.array(f).shape,np.array(d).shape)\n",
    "g = torch.tensor(d).bmm(torch.tensor(f))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, epoches,learning_rate=0.0001,print_every=7):\n",
    "    start = time.time()\n",
    "    # print_loss_total = 0  # Reset every print_every\n",
    " #  plot_loss_total = 0  # Reset every plot_every\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer,mode='min',factor=0.1,\n",
    "patience=60,verbose=True,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-015)\n",
    "    #criterion = nn.MSELoss(reduction='sum')\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_min = np.inf\n",
    "    train_losses =[]\n",
    "    valid_losses =[]\n",
    "    for e in range(1, epoches + 1):\n",
    "        train_loss = []\n",
    "        for batch_i,(local_batch, local_labels) in enumerate(Training_generator):\n",
    "            #if local_batch.shape[0]!=BatchSize:\n",
    "             #   continue\n",
    "            encoder.zero_grad()\n",
    "            local_batch = local_batch.to(device)\n",
    "            local_labels = local_labels.to(device)\n",
    "            \n",
    "            predY = encoder(local_batch)\n",
    "            loss = criterion(predY,local_labels).to(device)\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            if batch_i % print_every == 0:     \n",
    "                valid_loss = []\n",
    "                encoder.eval()\n",
    "                for x,y in Valid:\n",
    "                    x,y = x.to(device),y.to(device)\n",
    "                    predict = encoder(x)\n",
    "                    loss_valid = criterion(predict,y)\n",
    "                    valid_loss.append(loss_valid.item())\n",
    "                encoder.train()\n",
    "                train_loss_mean = np.mean(train_loss)\n",
    "                valid_loss_mean = np.mean(valid_loss)\n",
    "                train_losses.append(train_loss_mean)\n",
    "                valid_losses.append(valid_loss_mean)\n",
    "                scheduler.step(valid_loss_mean)\n",
    "                print(\"Epoch:{}/{},Step:{}/{}\".format(e,epoches,batch_i,len(Training_generator)),\n",
    "                  \"Train_Loss:{},Valid_Loss: {}\".format(train_loss_mean,valid_loss_mean))\n",
    "                if valid_loss_mean < loss_min :                             \n",
    "                    print(\"valid_loss decrease!!!save the model.\")\n",
    "                    loss_min = valid_loss_mean\n",
    "                    torch.save(encoder.state_dict(),'model/trajectory_predict_improve_8_4.pt')\n",
    "                torch.save(encoder.state_dict(),'model/trajectory_predict_improve_8_4_each.pt')\n",
    "    plt.plot(train_losses,color = 'r',label = 'Train_Loss')\n",
    "    plt.plot(valid_losses,color = 'g',label = 'Valid_Loss')\n",
    "    plt.title('Loss_Trend')\n",
    "    plt.xlabel('Epoches')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.title('损失变化')\n",
    "    #plt.xlabel('迭代次数')\n",
    "    #plt.ylabel('损失大小')\n",
    "    plt.legend()\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.savefig('image/trajectory_loss_improve_8_4.svg',dpi=600)\n",
    "    plt.savefig('image/trajectory_loss_improve_8_4.png',dpi=600)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 150, 8]) torch.Size([128, 150, 4])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(Training_generator)\n",
    "x, y = train_iter.next()\n",
    "print(x.shape,y.shape)\n",
    "hidden_size = 256\n",
    "Prednet = NNPred(x.shape[2], y.shape[2],hidden_size, x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/80,Step:0/188 Train_Loss:0.00184688407252147,Valid_Loss: 0.0022490793942894207\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:20/188 Train_Loss:0.002993640158565606,Valid_Loss: 0.0022417034274962726\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:40/188 Train_Loss:0.002565542235788857,Valid_Loss: 0.0022345205326274706\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:60/188 Train_Loss:0.0027065878271282763,Valid_Loss: 0.002226409803851501\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:80/188 Train_Loss:0.002534621017622754,Valid_Loss: 0.0022163997471713356\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:100/188 Train_Loss:0.0027186017954901566,Valid_Loss: 0.002226108970940732\n",
      "Epoch:1/80,Step:120/188 Train_Loss:0.002793658657056292,Valid_Loss: 0.0022234907936746844\n",
      "Epoch:1/80,Step:140/188 Train_Loss:0.0030746043727342353,Valid_Loss: 0.0022060516412402255\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:1/80,Step:160/188 Train_Loss:0.002998383374894783,Valid_Loss: 0.0022200409414719507\n",
      "Epoch:1/80,Step:180/188 Train_Loss:0.0032413125599539856,Valid_Loss: 0.002223877487787879\n",
      "Epoch:2/80,Step:0/188 Train_Loss:0.002145620477228226,Valid_Loss: 0.0022411146262544036\n",
      "Epoch:2/80,Step:20/188 Train_Loss:0.002043070003919677,Valid_Loss: 0.0022148789673095436\n",
      "Epoch:2/80,Step:40/188 Train_Loss:0.0023765506884796984,Valid_Loss: 0.002218830008610038\n",
      "Epoch:2/80,Step:60/188 Train_Loss:0.0025111656853689743,Valid_Loss: 0.0022196213276530576\n",
      "Epoch:2/80,Step:80/188 Train_Loss:0.003045996865283779,Valid_Loss: 0.0022310458124121216\n",
      "Epoch:2/80,Step:100/188 Train_Loss:0.0029445869073215416,Valid_Loss: 0.0022253598367287346\n",
      "Epoch:2/80,Step:120/188 Train_Loss:0.003430564966283842,Valid_Loss: 0.0022207397168192426\n",
      "Epoch:2/80,Step:140/188 Train_Loss:0.0032630820639195112,Valid_Loss: 0.0022366962112721185\n",
      "Epoch:2/80,Step:160/188 Train_Loss:0.003124498003569079,Valid_Loss: 0.0022328411077612727\n",
      "Epoch:2/80,Step:180/188 Train_Loss:0.003275449443535698,Valid_Loss: 0.0022793880214026824\n",
      "Epoch:3/80,Step:0/188 Train_Loss:0.0019612167569076945,Valid_Loss: 0.002249876125943243\n",
      "Epoch:3/80,Step:20/188 Train_Loss:0.002900764708421848,Valid_Loss: 0.002241192272247103\n",
      "Epoch:3/80,Step:40/188 Train_Loss:0.002553538949928376,Valid_Loss: 0.002234088188778807\n",
      "Epoch:3/80,Step:60/188 Train_Loss:0.002620079731250536,Valid_Loss: 0.002234982018705195\n",
      "Epoch:3/80,Step:80/188 Train_Loss:0.0024890756749976666,Valid_Loss: 0.0022348206793452005\n",
      "Epoch:3/80,Step:100/188 Train_Loss:0.0024480110820090796,Valid_Loss: 0.0022270286648627493\n",
      "Epoch:3/80,Step:120/188 Train_Loss:0.002882563829320879,Valid_Loss: 0.002219093896443607\n",
      "Epoch:3/80,Step:140/188 Train_Loss:0.003170716503614663,Valid_Loss: 0.0022264936878289973\n",
      "Epoch:3/80,Step:160/188 Train_Loss:0.0033431830193598345,Valid_Loss: 0.0022182456231246346\n",
      "Epoch:3/80,Step:180/188 Train_Loss:0.003278240432300008,Valid_Loss: 0.002230221535277865\n",
      "Epoch:4/80,Step:0/188 Train_Loss:0.00258089608505024,Valid_Loss: 0.002220735027050199\n",
      "Epoch:4/80,Step:20/188 Train_Loss:0.004652155198880657,Valid_Loss: 0.0022380548963149803\n",
      "Epoch:4/80,Step:40/188 Train_Loss:0.005205703133827651,Valid_Loss: 0.0022375433563624108\n",
      "Epoch:4/80,Step:60/188 Train_Loss:0.004204890148129681,Valid_Loss: 0.0022674980832287687\n",
      "Epoch:4/80,Step:80/188 Train_Loss:0.003835610786014988,Valid_Loss: 0.0022527819244885245\n",
      "Epoch:4/80,Step:100/188 Train_Loss:0.0038374463157280422,Valid_Loss: 0.0022429303034436437\n",
      "Epoch:4/80,Step:120/188 Train_Loss:0.0035450275816319727,Valid_Loss: 0.002233161447487124\n",
      "Epoch:4/80,Step:140/188 Train_Loss:0.003553640015793596,Valid_Loss: 0.0022485114648784174\n",
      "Epoch:4/80,Step:160/188 Train_Loss:0.0034214326608390426,Valid_Loss: 0.002226457140286709\n",
      "Epoch:4/80,Step:180/188 Train_Loss:0.003273269698818392,Valid_Loss: 0.002246810871596719\n",
      "Epoch:5/80,Step:0/188 Train_Loss:0.00199675755112757,Valid_Loss: 0.0022514961567591707\n",
      "Epoch:5/80,Step:20/188 Train_Loss:0.0058249574892372475,Valid_Loss: 0.0022334862618570684\n",
      "Epoch:5/80,Step:40/188 Train_Loss:0.005285102485799487,Valid_Loss: 0.0022448749876375703\n",
      "Epoch:5/80,Step:60/188 Train_Loss:0.004444766293118117,Valid_Loss: 0.0023485099375937005\n",
      "Epoch:5/80,Step:80/188 Train_Loss:0.003865364912825567,Valid_Loss: 0.0022374180317986723\n",
      "Epoch:5/80,Step:100/188 Train_Loss:0.0035310960918177987,Valid_Loss: 0.002247475454734057\n",
      "Epoch:5/80,Step:120/188 Train_Loss:0.00328364776538558,Valid_Loss: 0.0022467308205287063\n",
      "Epoch:5/80,Step:140/188 Train_Loss:0.003108978098551732,Valid_Loss: 0.0022426568101487623\n",
      "Epoch:5/80,Step:160/188 Train_Loss:0.003277873978420539,Valid_Loss: 0.002370925001184025\n",
      "Epoch:5/80,Step:180/188 Train_Loss:0.003271413900319507,Valid_Loss: 0.00222937897376444\n",
      "Epoch:6/80,Step:0/188 Train_Loss:0.002089049991641973,Valid_Loss: 0.0022365367665291677\n",
      "Epoch:6/80,Step:20/188 Train_Loss:0.004501620989885884,Valid_Loss: 0.002250767162673615\n",
      "Epoch:6/80,Step:40/188 Train_Loss:0.003578768157578126,Valid_Loss: 0.0022730203091113664\n",
      "Epoch:6/80,Step:60/188 Train_Loss:0.003785016460194004,Valid_Loss: 0.002241979055247579\n",
      "Epoch:6/80,Step:80/188 Train_Loss:0.003697275751424763,Valid_Loss: 0.0023847330742202823\n",
      "Epoch:6/80,Step:100/188 Train_Loss:0.003596660182253219,Valid_Loss: 0.00237333892564553\n",
      "Epoch:6/80,Step:120/188 Train_Loss:0.003373797115057196,Valid_Loss: 0.0023695123548382355\n",
      "Epoch:6/80,Step:140/188 Train_Loss:0.003182338584886122,Valid_Loss: 0.002247159255960173\n",
      "Epoch:6/80,Step:160/188 Train_Loss:0.0033676874184686606,Valid_Loss: 0.0022585545113808507\n",
      "Epoch:6/80,Step:180/188 Train_Loss:0.0032667897392616068,Valid_Loss: 0.0022789182954964554\n",
      "Epoch:7/80,Step:0/188 Train_Loss:0.0020363816185865833,Valid_Loss: 0.0022561761998330905\n",
      "Epoch:7/80,Step:20/188 Train_Loss:0.0024622876952906537,Valid_Loss: 0.0022405267527456237\n",
      "Epoch:7/80,Step:40/188 Train_Loss:0.0023120962668713046,Valid_Loss: 0.0022388452938840336\n",
      "Epoch:7/80,Step:60/188 Train_Loss:0.002249090539665808,Valid_Loss: 0.002236473061349195\n",
      "Epoch:7/80,Step:80/188 Train_Loss:0.002207231144656548,Valid_Loss: 0.0022159615206540227\n",
      "Epoch:7/80,Step:100/188 Train_Loss:0.002479023922158903,Valid_Loss: 0.002236356468292962\n",
      "Epoch:7/80,Step:120/188 Train_Loss:0.0028265544085517486,Valid_Loss: 0.0022226131926275723\n",
      "Epoch:7/80,Step:140/188 Train_Loss:0.003226533612202476,Valid_Loss: 0.002245691502863302\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:7/80,Step:160/188 Train_Loss:0.0030994069181557557,Valid_Loss: 0.0022356763692920867\n",
      "Epoch:7/80,Step:180/188 Train_Loss:0.0029988902064659348,Valid_Loss: 0.0022258244011809478\n",
      "Epoch:8/80,Step:0/188 Train_Loss:0.0019519774725870983,Valid_Loss: 0.002233814325467246\n",
      "Epoch:8/80,Step:20/188 Train_Loss:0.0027014830111603888,Valid_Loss: 0.0022164619703054877\n",
      "Epoch:8/80,Step:40/188 Train_Loss:0.0024501558130825564,Valid_Loss: 0.0022192588563824107\n",
      "Epoch:8/80,Step:60/188 Train_Loss:0.002594391040634553,Valid_Loss: 0.0022179560784913433\n",
      "Epoch:8/80,Step:80/188 Train_Loss:0.0024498642801412287,Valid_Loss: 0.00222137577072368\n",
      "Epoch:8/80,Step:100/188 Train_Loss:0.0025382895840440705,Valid_Loss: 0.0022187771633058032\n",
      "Epoch:8/80,Step:120/188 Train_Loss:0.0032934346732076303,Valid_Loss: 0.0022271679425085456\n",
      "Epoch:8/80,Step:140/188 Train_Loss:0.0031757316669060293,Valid_Loss: 0.0022361095509261723\n",
      "Epoch:8/80,Step:160/188 Train_Loss:0.003094939888972471,Valid_Loss: 0.0022233758356504257\n",
      "Epoch:8/80,Step:180/188 Train_Loss:0.0032515605480205483,Valid_Loss: 0.0023299321784642437\n",
      "Epoch:9/80,Step:0/188 Train_Loss:0.0019430489329735291,Valid_Loss: 0.0022179015418623753\n",
      "Epoch:9/80,Step:20/188 Train_Loss:0.004487117441406742,Valid_Loss: 0.0022110688083630055\n",
      "Epoch:9/80,Step:40/188 Train_Loss:0.005227431176387445,Valid_Loss: 0.00221595895762177\n",
      "Epoch:9/80,Step:60/188 Train_Loss:0.004439721096943418,Valid_Loss: 0.0022224191728094046\n",
      "Epoch:9/80,Step:80/188 Train_Loss:0.003966590592424126,Valid_Loss: 0.002211406333573687\n",
      "Epoch:9/80,Step:100/188 Train_Loss:0.003855040473243011,Valid_Loss: 0.0022211039502682433\n",
      "Epoch:9/80,Step:120/188 Train_Loss:0.0037490972863220818,Valid_Loss: 0.0022024248149847487\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:9/80,Step:140/188 Train_Loss:0.003548177675916663,Valid_Loss: 0.002213618907136078\n",
      "Epoch:9/80,Step:160/188 Train_Loss:0.0033847747660922184,Valid_Loss: 0.0022050245306759722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/80,Step:180/188 Train_Loss:0.0032444423471313416,Valid_Loss: 0.0023301030950933902\n",
      "Epoch:10/80,Step:0/188 Train_Loss:0.0019402532599551204,Valid_Loss: 0.0022061030259237016\n",
      "Epoch:10/80,Step:20/188 Train_Loss:0.006903295186890951,Valid_Loss: 0.0022018841075096772\n",
      "valid_loss decrease!!!save the model.\n",
      "Epoch:10/80,Step:40/188 Train_Loss:0.00457437936770281,Valid_Loss: 0.0022128146095685927\n",
      "Epoch:10/80,Step:60/188 Train_Loss:0.0037461884034541496,Valid_Loss: 0.0022107026392083748\n",
      "Epoch:10/80,Step:80/188 Train_Loss:0.003664127793832991,Valid_Loss: 0.0022374610541318254\n",
      "Epoch:10/80,Step:100/188 Train_Loss:0.0033463495408076658,Valid_Loss: 0.002214386881303918\n",
      "Epoch:10/80,Step:120/188 Train_Loss:0.0031497591439917645,Valid_Loss: 0.002215361850806025\n",
      "Epoch:10/80,Step:140/188 Train_Loss:0.0034144280928335813,Valid_Loss: 0.002212184384499293\n",
      "Epoch:10/80,Step:160/188 Train_Loss:0.00325308899749465,Valid_Loss: 0.00220574820443992\n",
      "Epoch:10/80,Step:180/188 Train_Loss:0.0031795056420671807,Valid_Loss: 0.0022109294181497665\n",
      "Epoch:11/80,Step:0/188 Train_Loss:0.004660770440687156,Valid_Loss: 0.0022170213332695886\n",
      "Epoch:11/80,Step:20/188 Train_Loss:0.0052485033443127795,Valid_Loss: 0.0022049331845906896\n",
      "Epoch:11/80,Step:40/188 Train_Loss:0.0038505505525158862,Valid_Loss: 0.0022112523096631067\n",
      "Epoch:11/80,Step:60/188 Train_Loss:0.003262074906465619,Valid_Loss: 0.0022069151915012956\n",
      "Epoch:11/80,Step:80/188 Train_Loss:0.002964048233697254,Valid_Loss: 0.002205905123093952\n",
      "Epoch:11/80,Step:100/188 Train_Loss:0.0029255223816289466,Valid_Loss: 0.0022072089005605623\n",
      "Epoch:11/80,Step:120/188 Train_Loss:0.0028623228181063757,Valid_Loss: 0.0023258027530452005\n",
      "Epoch:11/80,Step:140/188 Train_Loss:0.003091840915891872,Valid_Loss: 0.0022300172983222557\n",
      "Epoch:11/80,Step:160/188 Train_Loss:0.0030408203779434748,Valid_Loss: 0.002218640319543398\n",
      "Epoch:11/80,Step:180/188 Train_Loss:0.0031949900191749693,Valid_Loss: 0.002236809091964593\n",
      "Epoch:12/80,Step:0/188 Train_Loss:0.0021170492866350392,Valid_Loss: 0.0022103415861507263\n",
      "Epoch:12/80,Step:20/188 Train_Loss:0.0020381401103366373,Valid_Loss: 0.0022137356899541087\n",
      "Epoch:12/80,Step:40/188 Train_Loss:0.004396171541137666,Valid_Loss: 0.002219520112774459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(device)\n",
    "\n",
    "TRAN_TAG = True\n",
    "if TRAN_TAG:\n",
    "    if path.exists(\"model/trajectory_predict_improve_8_4.pt\"):\n",
    "        Prednet.load_state_dict(torch.load('model/trajectory_predict_improve_8_4.pt'))\n",
    "    Prednet = Prednet.double()\n",
    "    Prednet = Prednet.to(device)\n",
    "    trainIters(Prednet, 80,0.00001,20)\n",
    "   # torch.save(Prednet.state_dict(), 'trajectory_predict.pt')\n",
    "   # showPlot(plot_lo0sses)\n",
    "else:\n",
    "    Prednet.load_state_dict(torch.load('model/trajectory_predict_improve_8_4.pt'))\n",
    "    Prednet = Prednet.double()\n",
    "    Prednet = Prednet.to(device)\n",
    "    Prednet.eval()\n",
    "    Eval_net(Prednet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_XY(predY):\n",
    "    #input: [batchsize len features]; features:[velx,vely,x,y]\n",
    "    '''\n",
    "    deltaY = v0*delta_t + 0.5* a *delta_t^2\n",
    "    a = (v - v0)/delta_t\n",
    "    vo\n",
    "    '''\n",
    "    vels = predY[:,:,0:2]\n",
    "    rst_xy = np.zeros(predY[:,:,0:2].shape)\n",
    "    rst_xy[:,:-30,:] = predY[:,:-30,2:4]\n",
    "    delta_t = 0.1\n",
    "   \n",
    "    for i in range(30):\n",
    "        a = (vels[:,-(30-i),:] - vels[:,-(31-i),:])/delta_t\n",
    "        delta_xy = vels[:,-(30-i),:]*vels[:,-(30-i),:]-vels[:,-(31-i),:]*vels[:,-(31-i),:]\n",
    "        delta_xy = delta_xy/(2*a)\n",
    "        rst_xy[:,-(30-i),:] = rst_xy[:,-(31-i),:] + delta_xy\n",
    "    \"\"\"\n",
    "    for i in range(1,100):\n",
    "        a = (vels[:,i,:] - vels[:,i-1,:])/delta_t\n",
    "        delta_xy = vels[:,i,:]*vels[:,i-1,:]-vels[:,i-1,:]*vels[:,i-1,:]\n",
    "        delta_xy = delta_xy/(2*a)\n",
    "        rst_xy[:,i,:] = rst_xy[:,i-1,:] + delta_xy\n",
    "       \"\"\"\n",
    "    t = rst_xy-predY[:,:,2:4]\n",
    "    #print(rst_xy)\n",
    "    #print(t[1,:,:])\n",
    "    return rst_xy\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval_net(encoder):\n",
    "    n_trajectory_batch = 0\n",
    "    loss = []\n",
    "    for local_batch, local_labels in Test:\n",
    "        n_trajectory_batch = n_trajectory_batch + 1\n",
    "        criterion = nn.MSELoss()\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "        predY = encoder(local_batch)\n",
    "        #print(WholeSet.std.repeat(BatchSize,100,1).shape)\n",
    "        test_loss = criterion(predY,local_labels)\n",
    "        loss.append(test_loss.item())\n",
    "        std = WholeSet.std.repeat(local_batch.shape[0],100,1)\n",
    "        std = std[:,:,:4].to(device)\n",
    "        mn = WholeSet.mn.repeat(local_batch.shape[0],100,1)\n",
    "        mn = mn[:,:,:4].to(device)\n",
    "        rg = WholeSet.range.repeat(local_batch.shape[0],100,1)\n",
    "        rg = rg[:,:,:4].to(device)\n",
    "        predY = (predY*(rg*std)+mn).detach().cpu()\n",
    "        pY = np.array(predY )\n",
    "        pY =  scipy.signal.savgol_filter(pY, window_length=51, polyorder=3,axis=1)\n",
    "        local_labels = (local_labels*(rg*std)+mn).detach().cpu()\n",
    "        Y = np.array(local_labels)\n",
    "        pY[:,:-30,:] = Y[:,:-30,:]\n",
    "        rst_xy = calcu_XY(pY)\n",
    "        if n_trajectory_batch > 20:\n",
    "            break\n",
    "        for i in range(1):\n",
    "            #plt.figure(i)\n",
    "            #plt.xlim(0,80)\n",
    "            #plt.ylim(0,2000)\n",
    "            #plt.subplot(4,5,n_trajectory_batch)\n",
    "            plt.plot(pY[i,:,2],pY[i,:,3],'r',label='prediction')\n",
    "            plt.plot(Y[i,:,2],Y[i,:,3],'g',label='actual')\n",
    "            plt.plot(rst_xy[i,:,0],rst_xy[i,:,1],'b',label='rst_xy')\n",
    "            plt.legend()\n",
    "        plt.show()\n",
    "        print('Test loss:',test_loss.item())\n",
    "    print('average loss:',np.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "Prednet = NNPred(x.shape[2], y.shape[2],hidden_size, x.shape[0])\n",
    "Prednet.load_state_dict(torch.load('model/trajectory_predict_improve_8_4.pt'))\n",
    "Prednet = Prednet.double()\n",
    "Prednet = Prednet.to(device)\n",
    "Prednet.eval()\n",
    "Eval_net(Prednet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_load,n):\n",
    "    test = iter(test_load)\n",
    "    x,y =  test.next()\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    predY = model(x)\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = criterion(predY,y)\n",
    "    std = WholeSet.std.repeat(x.shape[0],100,1)\n",
    "    std = std[:,:,:4].to(device)\n",
    "    mn = WholeSet.mn.repeat(x.shape[0],100,1)\n",
    "    mn = mn[:,:,:4].to(device)\n",
    "    rg = WholeSet.range.repeat(x.shape[0],100,1)\n",
    "    rg = rg[:,:,:4].to(device)\n",
    "    predY = (predY*(rg*std)+mn).detach().cpu()\n",
    "    pY = np.array(predY)\n",
    "    pY =  scipy.signal.savgol_filter(pY, window_length=51, polyorder=3,axis=1)\n",
    "    local_labels = (y*(rg*std)+mn).detach().cpu()\n",
    "    Y = np.array(local_labels)\n",
    "   # pY[:,:-30,:] = Y[:,:-30,:]\n",
    "    rst_xy = calcu_XY(pY)\n",
    "    #rst_xy =  scipy.signal.savgol_filter(rst_xy, window_length=31, polyorder=3,axis=1)\n",
    "    #rst_xy[:,-30:-1,:]=  scipy.signal.savgol_filter(rst_xy[:,-30:-1,:], window_length=51, polyorder=3,axis=0)\n",
    "    plt.plot(rst_xy[:n,:,0][0],rst_xy[:n,:,1][0],'b',label=\"rst_xy\")\n",
    "    plt.plot(pY[:n,:,2][0],pY[:n,:,3][0],'r',label='prediction')\n",
    "    plt.plot(Y[:n,:,2][0],Y[:n,:,3][0],'g',label='actual')\n",
    "    plt.legend()\n",
    "    print(\"Test Loss:\",test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "Prednet = NNPred(x.shape[2], y.shape[2],hidden_size, x.shape[0])\n",
    "Prednet.load_state_dict(torch.load('model/trajectory_predict_improve_8_4.pt'))\n",
    "Prednet = Prednet.double()\n",
    "Prednet = Prednet.to(device)\n",
    "Prednet.eval()\n",
    "predict(Prednet,Test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(Test)\n",
    "x, y = test_iter.next()\n",
    "print(x.shape)\n",
    "x,y = x.to(device),y.to(device)\n",
    "x = x[0][0].view(-1,len(x[0][0])).unsqueeze(0)\n",
    "predY = Prednet(x)\n",
    "print(predY)\n",
    "print(y[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_XY_point(predY,size):\n",
    "    #input: [batchsize len features]; features:[velx,vely,x,y]\n",
    "    '''\n",
    "    deltaY = v0*delta_t + 0.5* a *delta_t^2\n",
    "    a = (v - v0)/delta_t\n",
    "    vo\n",
    "    '''\n",
    "    vels = predY[:,:,0:2]\n",
    "    rst_xy = np.zeros(predY[:,:,0:2].shape)\n",
    "    rst_xy[:,:-size,:] = predY[:,:-size,2:4]\n",
    "    delta_t = 0.1\n",
    "   \n",
    "    for i in range(size):\n",
    "        a = (vels[:,-(size-i),:] - vels[:,-(size+1-i),:])/delta_t\n",
    "        delta_xy = vels[:,-(size-i),:]*vels[:,-(size-i),:]-vels[:,-(size+1-i),:]*vels[:,-(size+1-i),:]\n",
    "        delta_xy = delta_xy/(2*a)\n",
    "        rst_xy[:,-(size-i),:] = rst_xy[:,-(size+1-i),:] + delta_xy\n",
    "    \"\"\"\n",
    "    for i in range(1,100):\n",
    "        a = (vels[:,i,:] - vels[:,i-1,:])/delta_t\n",
    "        delta_xy = vels[:,i,:]*vels[:,i-1,:]-vels[:,i-1,:]*vels[:,i-1,:]\n",
    "        delta_xy = delta_xy/(2*a)\n",
    "        rst_xy[:,i,:] = rst_xy[:,i-1,:] + delta_xy\n",
    "       \"\"\"\n",
    "    return rst_xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavior_model import LSTM\n",
    "def compute_label(py):\n",
    "    net_behavior = LSTM(1,5,256,2) \n",
    "    net_behavior.load_state_dict(torch.load('model/lstm_behavior_prediction_old.pt'))\n",
    "    net_behavior.to(device)\n",
    "    inputs = torch.from_numpy(py[:,-1,:-1].unsqueeze(2).detach().cpu().numpy().astype(np.float32)).to(device)\n",
    "    h = net_behavior.init_hidden(py[:,-1,:-1].shape[0])\n",
    "    h = tuple([each.data for each in h])\n",
    "    behavior,h = net_behavior(inputs,h)\n",
    "    _, class_ = torch.max(behavior, dim=1)\n",
    "    class_ = class_.reshape(-1,1,1)\n",
    "    class_ = torch.from_numpy(class_.detach().cpu().numpy().astype(np.double)).to(device)\n",
    "    std = WholeSet.std.repeat(x.shape[0],1,1)\n",
    "    std = std[:,:,-1].unsqueeze(1).to(device)\n",
    "    mn = WholeSet.mn.repeat(x.shape[0],1,1)\n",
    "    mn = mn[:,:,-1].unsqueeze(1).to(device)\n",
    "    rg = WholeSet.range.repeat(x.shape[0],1,1)\n",
    "    rg = rg[:,:,-1].unsqueeze(1).to(device)\n",
    "    class_ = (class_ -mn) / (std*rg)\n",
    "    py = torch.cat((py[:,-1,:-1].unsqueeze(1),class_),2)\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_iter.next()\n",
    "x,y = x.to(device),y.to(device)\n",
    "x = x[:,:50,:]\n",
    "predY = x\n",
    "py = predict_point(Prednet,predY)\n",
    "print(compute_label(py).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavior_model import LSTM\n",
    "\n",
    "def predict_point(net,x):\n",
    "    predY = net(x)\n",
    "    return predY\n",
    "\n",
    "def predict_trajectory(net,hoistory_length,size,test_load,n=0):\n",
    "    test_iter = iter(test_load)\n",
    "    x, y = test_iter.next()\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    x = x[:,:hoistory_length,:]\n",
    "    predY = x\n",
    "    for ii in range(size):\n",
    "        py = predict_point(net,predY)\n",
    "        py = compute_label(py)\n",
    "        predY = torch.cat((predY,py),1)\n",
    "         #predY = torch.cat((predY,py[:,-1,:].unsqueeze(1)),1)\n",
    "        predY = torch.from_numpy(scipy.signal.savgol_filter(predY.detach().cpu().numpy(), window_length=hoistory_length-11, polyorder=3,axis=1)).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = criterion(predY,y[:,0:size+hoistory_length,:])\n",
    "    std = WholeSet.std.repeat(x.shape[0],size+hoistory_length,1)\n",
    "    std = std[:,:,:].to(device)\n",
    "    mn = WholeSet.mn.repeat(x.shape[0],size+hoistory_length,1)\n",
    "    mn = mn[:,:,:].to(device)\n",
    "    rg = WholeSet.range.repeat(x.shape[0],size+hoistory_length,1)\n",
    "    rg = rg[:,:,:].to(device)\n",
    "    predY = (predY*(rg*std)+mn).detach().cpu()\n",
    "    pY = np.array(predY)\n",
    "    pY =  scipy.signal.savgol_filter(pY, window_length=hoistory_length+size-11, polyorder=3,axis=1)\n",
    "    local_labels = (y[:,0:hoistory_length+size,:]*(rg*std)+mn).detach().cpu()\n",
    "    Y = np.array(local_labels)\n",
    "    pY[:,:hoistory_length,:] = Y[:,:hoistory_length,:]\n",
    "    rst_xy = calcu_XY_point(pY,size)\n",
    "    #rst_xy =  scipy.signal.savgol_filter(rst_xy, window_length=31, polyorder=3,axis=1)\n",
    "    #rst_xy[:,-30:-1,:]=  scipy.signal.savgol_filter(rst_xy[:,-30:-1,:], window_length=51, polyorder=3,axis=0)\n",
    "   # for i in range(10):\n",
    "        #plt.subplot(4,5,i+1)\n",
    "    plt.plot(pY[n,:hoistory_length,2],pY[n,:hoistory_length,3],'y',label='History Trajectory')\n",
    "    plt.plot(rst_xy[n,hoistory_length:,0],rst_xy[n,hoistory_length:,1],'b',label=\"rst_xy\")\n",
    "    plt.plot(pY[n,hoistory_length:,2],pY[n,hoistory_length:,3],'r',label='prediction')\n",
    "    plt.plot(Y[n,hoistory_length:,2],Y[n,hoistory_length:,3],'g',label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_trajectory(Prednet,20,20,Test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
